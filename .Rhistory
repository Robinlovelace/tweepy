language, n_followers, user_created, n_tweets, n_followers, n_following,
user_location)
# sel <- grepl("new house|new flat|moving house|move house|moving home|move home|#newhouse|#newflat|#movinghouse|#movehouse|#movinghome|#movehome", t_out$text, ignore.case = T ) # original selection
# sel <- grepl("a", t_out$text, ignore.case = T ) # test selection - replace "a" with anything
sel <- grepl("new house|#newhouse|old house|#oldhouse|new home|#newhome|old home|#oldhome|new flat|#newflat|old flat|#oldflat|moving house|#movinghouse|move house|#movehouse|moving home|#movinghome|move home|#movehome|packing to move|packing up everything|unpacking everything|removals van|#packingtomove|#packingupeverything|#unpackingeverything|#removalsvan|bought a house|house bought|moved house|house sold|#boughtahouse|#housebought|#movedhouse|#housesold|first rent|#firstrent|new gaff|new housing|new accommodation|new crib|new bungalow|new apartment|new semi detached|new semi-detached|new detached|new cottage|new digs|new dwelling|new residence|new pad|new homes|new home's|new houses|new house's|#newgaff|#newhousing|#newaccommodation|#newcrib|#newbungalow|#newapartment|#newsemidetached|#newdetached|#newcottage|#newdigs|#newdwelling|#newresidence|#newpad|#newhomes|#newhouses|old gaff|old housing|old accommodation|old crib|old bungalow|old apartment|old semi detached|old semi-detached|old detached|old cottage|old digs|old dwelling|old residence|old pad|old homes|old home's|old houses|old house's|#oldgaff|#oldhousing|#oldaccommodation|#oldcrib|#oldbungalow|#oldapartment|#oldsemidetached|#olddetached|#oldcottage|#olddigs|#olddwelling|#oldresidence|#oldpad|#oldhomes|#oldhouses", t_out$text, ignore.case = T)
t_out$filenum <- which(files == i)
write.csv(t_out[sel, ], file = paste0("data/output",which(files == i),".csv"))
print(paste0(which(files == i) / length(files) * 100, "% done"))
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
output
for(j in outs[-1]){
output <- rbind(output, read.csv(j))
}
which(outs == j)
for(j in outs[-1]){
tryCatch(
output <- rbind(output, read.csv(j)))
num <- which(outs == j)
}
?tryCatch
which(i == files)
which(i = file)
which(i == file)
which(i == files)
i
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
summary(output)
write.csv(output, "output.csv")
source("analysis/geosel.R")
source("analysis/geosel.R")
geoT[geosel, ]
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
geoT[geosel, ]
source("analysis/geosel.R")
geoT[geosel, ]
geosel <- gBuffer(pw, width = 50000) # create buffer
plot(geosel) # plot to test dimensions make sense
geosel <- spTransform(geosel, CRS("+init=epsg:4326"))
proj4string(geoT) <- CRS("+init=epsg:4326")
geoT[geosel, ]
geoT@data[geosel, ]
geoT[geosel, ]@data
nrow(t_out)
nlines <- 0
sel <- grepl("pennine way", t_out$text, ignore.case = T ) # test selection
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
source("analysis/geosel.R")
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
proj4string(geoT) <- CRS("+init=epsg:4326")
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
t_out$filenum <- which(files == i)
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
sel <- grepl("pennine way", t_out$text, ignore.case = T ) # test selection
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
t_sel
source("analysis/geosel.R")
sel <- grepl("pennine way", t_out$text, ignore.case = T ) # test selection
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
t_sel
# system("mkdir data/chunked") # copy chunked pieces into one directory
# source("analysis/geosel.R")
library(rgdal)
ogrListLayers("pennine.gpx")
pw <- readOGR("pennine.gpx", layer = "tracks")
pw <- spTransform(pw, CRS("+init=epsg:27700")) # transform CRS to OSGB
library(rgeos)
geosel <- gBuffer(pw, width = 5000) # create buffer
plot(geosel) # plot to test dimensions make sense
geosel <- spTransform(geosel, CRS("+init=epsg:4326"))
# Setup
library(rjson) # library used to load .json files
files <- list.files(path = "data/chunked/", full.names=T)
# i <- files[1] # uncomment to load 1
start_time <- Sys.time()
nlines <- 0
for(i in files){
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
user_location)
### geo selection part
sel <- grepl("pennine way", t_out$text, ignore.case = T ) # test selection
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_sel <- rbind(geoT[geosel, ]@data, t_out[sel, ])
t_out$filenum <- which(files == i)
write.csv(t_sel, file = paste0("data/output",which(files == i),".csv"))
print(paste0(which(files == i) / length(files) * 100, "% done"))
nlines <- nlines + nrow(t_out)
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
summary(output)
write.csv(output, "output-pennine.csv")
nrow(output)
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
summary(output)
tweets <- fromJSON(sprintf("[%s]", paste(readLines("data/unzipped/t1402075195564.json"), collapse=",")))
library(rjson) # library used to load .json files
tweets <- fromJSON(sprintf("[%s]", paste(readLines("data/unzipped/t1402075195564.json"), collapse=",")))
x <- c("a", "ab", "ac")
grep("ab", x)
grep("a(cb)", x)
grep("a\(cb\)", x)
grep("a(c:b)", x)
grep("a(c:b)", x)
?grep
grep("a[cb]", x)
files <- list.files(path = "data/chunked/", full.names=T)
i <- files[1] #
i
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
library(rjson) # library used to load .json files
pw <- readOGR("data/", "study-area")
library(rgdal)
pw <- readOGR("data/", "study-area")
pw <- readOGR("~/Dropbox/NARSC Submission/data/", "study-area")
pw <- readOGR("~/Dropbox/NARSC Submission/data/", "study-area")
library(rgdal)
pw <- readOGR("~/Dropbox/NARSC Submission/data/", "study-area")
pw <- readOGR("~/Dropbox/NARSC Submission/data", "study-area")
pw <- readOGR("/home/georl/Dropbox/NARSC Submission/data", "study-area")
pw <- spTransform(pw, CRS("+init=epsg:27700")) # transform CRS to OSGB
pw <- spTransform(pw, CRS("+init=epsg:4326")) # transform CRS to OSGB
library(rgeos)
geosel <- spTransform(pw, CRS("+init=epsg:4326"))
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
coords
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
tweets[[1]]$geo$type
tweets[[1]]$geo$type
tweets[[1]]$geo$coordinates
tweets[[1]]$coordinates
tweets[[1]]$coordinates$coordinates
tweets[[1]]$geo$coordinates
pw <- readOGR("~/Dropbox/NARSC Submission/", "study-area")
pw <- spTransform(pw, CRS("+init=epsg:27700")) # transform CRS to OSGB
library(rgeos)
geosel <- spTransform(pw, CRS("+init=epsg:4326"))
plot(geosel)
tweets[[1]]$retweet_count
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
n_rts <- sapply(tweets, function(x) x$retweet_count)
n_fav <- sapply(tweets, function(x) x$favorite_count)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
n_fav, n_rts, user_location)
### geo selection part
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_out <- geoT[geosel, ]@data
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_sel <- t_out[sel, ])
t_out$filenum <- which(files == i)
write.csv(t_sel, file = paste0("data/output",which(files == i),".csv"))
print(paste0(which(files == i) / length(files) * 100, "% done"))
nlines <- nlines + nrow(t_out)
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
t_sel <- t_out[sel, ]
t_out <- geoT[geosel, ]@data
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_out <- t_out[sel, ]
summary(t_out)
plot(geoT[geosel,])
# Setup
library(rjson) # library used to load .json files
files <- list.files(path = "data/chunked/", full.names=T)
# i <- files[1] # uncomment to load 1
start_time <- Sys.time()
nlines <- 0
for(i in files){
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
n_rts <- sapply(tweets, function(x) x$retweet_count)
n_fav <- sapply(tweets, function(x) x$favorite_count)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
n_fav, n_rts, user_location)
### geo selection part
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_out <- geoT[geosel, ]@data
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_out <- t_out[sel, ]
t_out$filenum <- which(files == i)
write.csv(t_sel, file = paste0("data/output",which(files == i),".csv"))
print(paste0(which(files == i) / length(files) * 100, "% done"))
nlines <- nlines + nrow(t_out)
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
summary(output)
write.csv(output, "output-retail.csv")
which(files == i)
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
n_rts <- sapply(tweets, function(x) x$retweet_count)
n_fav <- sapply(tweets, function(x) x$favorite_count)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
n_fav, n_rts, user_location)
### geo selection part
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_out <- geoT[geosel, ]@data
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_out <- t_out[sel, ]
t_out$filenum <- which(files == i)
t_out$filenum <- which(files == i)
t_out$filenum
filenum <- which(files == i)
if( i %% 6 == 0 ) print(paste0(which(files == i) / length(files) * 100, "% done"))
if( which(files == i) %% 6 == 0 ) print(paste0(which(files == i) / length(files) * 100, "% done"))
if( which(files == i) %% 10 == 0 ) print(paste0(which(files == i) / length(files) * 100, "% done"))
library(rgdal)
pw <- readOGR("~/Dropbox/NARSC Submission/", "study-area")
pw <- spTransform(pw, CRS("+init=epsg:27700")) # transform CRS to OSGB
library(rgeos)
geosel <- spTransform(pw, CRS("+init=epsg:4326"))
tweets[[1]]$coordinates$coordinates
tweets[[1]]$geo$coordinates
tweets[[1]]$favorite_count
# Setup
library(rjson) # library used to load .json files
files <- list.files(path = "data/chunked/", full.names=T)
# i <- files[1] # uncomment to load 1
start_time <- Sys.time()
nlines <- 0
for(i in files){
# tweets <- fromJSON(sprintf("[%s]", paste(readLines(i, n=1000), collapse=","))) # test subset
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text <- sapply(tweets, function(x) x$text )
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
n_rts <- sapply(tweets, function(x) x$retweet_count)
n_fav <- sapply(tweets, function(x) x$favorite_count)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
n_fav, n_rts, user_location)
### geo selection part
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_out <- geoT[geosel, ]@data
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_out <- t_out[sel, ]
filenum <- which(files == i)
write.csv(t_sel, file = paste0("data/output",which(files == i),".csv"))
if( filenum %% 10 == 0 ) print(paste0(which(files == i) / length(files) * 100, "% done"))
nlines <- nlines + nrow(t_out)
}
end_time <- Sys.time()
(time_taken <- end_time - start_time)
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
summary(output)
write.csv(output, "output-retail.csv")
output$text[1:20]
tryCatch({
tweets <- fromJSON(sprintf("[%s]", paste(readLines(i), collapse=","))) # full dataset
}, error=function(e){paste0("Error ", which(i == files))})
coords <- sapply(tweets, function(x) x$coordinates$coordinates )
nuls <- sapply(coords, function(x) is.null(x)) # identify out the problematic NULL values
coords[nuls] <- lapply(coords[nuls], function(x) x <- c(0, 0)) # convert to zeros to keep with unlist
coords <- matrix(unlist(coords, recursive = T), ncol = 2, byrow = T)
text
created <- sapply(tweets, function(x) x$created_at)
created <- strptime(created, "%a %b %d %H:%M:%S +0000 %Y")
language <- sapply(tweets, function(x) x$lang )
n_followers <- sapply(tweets, function(x) x$lang )
user_created <- sapply(tweets, function(x) x$user$created_at)
n_tweets <- sapply(tweets, function(x) x$user$statuses_count)
n_followers <- sapply(tweets, function(x) x$user$followers_count)
n_following <- sapply(tweets, function(x) x$user$friends_count)
user_location <- sapply(tweets, function(x) x$user$location)
n_rts <- sapply(tweets, function(x) x$retweet_count)
n_fav <- sapply(tweets, function(x) x$favorite_count)
user_description <- sapply(tweets, function(x) x$user$description)
user_id <- sapply(tweets, function(x) x$user$id)
user_idstr <- sapply(tweets, function(x) x$user$id_str)
user_name <- sapply(tweets, function(x) x$user$name)
user_screen_name <- sapply(tweets, function(x) x$user$screen_name)
t_out <- data.frame(text, lat = coords[,2], lon = coords[,1], created,
language, n_followers, user_created, n_tweets, n_followers, n_following,
n_fav, n_rts, user_location)
t_out
geoT <- SpatialPointsDataFrame(coords= matrix(c(t_out$lon, t_out$lat), ncol=2), data=t_out)
plot(geoT)
proj4string(geoT) <- CRS("+init=epsg:4326")
t_out <- geoT[geosel, ]@data
t_out
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", geoT$text, ignore.case = T ) # selection
t_out[sel, ]
t_out
write.csv(t_sel, file = paste0("data/output",which(files == i),".csv"))
read.csv(j)
write.csv(t_out, file = paste0("data/output",which(files == i),".csv"))
if( filenum %% 100 == 0 ) print(paste0(which(files == i) / length(files) * 100, "% done"))
nlines <- nlines + nrow(t_out)
}
read.csv(j)
t_out
which(files == i)
j
read.csv("data/output2656.csv")
write.csv(t_out, file = paste0("data/output",which(files == i),".csv"), )
read.csv("data/output2656.csv")
t_out
sel
sel <- grepl("shop|buy|bought|expensive|cheap|bargain|store|visit", t_out$text, ignore.case = T ) # selection
sel
outs <- list.files(path = "data/", pattern=".csv", full.names=T)
output <- read.csv(outs[1])
for(j in outs[-1]){
tryCatch({
output <- rbind(output, read.csv(j))
}, error=function(e){paste0("Error ", which(outs == j))})
num <- which(outs == j)
}
download.file("https://geoportal.statistics.gov.uk/Docs/Boundaries/Workplace_zones_(E+W)_2011_Boundaries_(Full_Extent).zip", destfile = "/media/SAMSUNG/data/2011-census/wp.zip")
download.file("https://geoportal.statistics.gov.uk/Docs/Boundaries/Travel_to_work_areas_(E+W)_2007_Boundaries_(Generalised_Clipped).zip", destfile = "/tmp/ttwz.zip")
download.file("https://geoportal.statistics.gov.uk/Docs/Boundaries/Workplace_zones_(E+W)_2011_Boundaries_(Full_Extent).zip", destfile = "/media/SAMSUNG/data/2011-census/ttwz.zip")
ct <- read.csv("output.csv", as.is = TRUE)
ct <- read.csv("output.csv", as.is = TRUE)
install.packages("wordcloud")
ct <- read.csv("~/repos/tweepy/Carnival/output.csv", as.is = TRUE)
nrow(ct)
source('~/.active-rstudio-document', echo=TRUE)
wordcloud(unique_words, freq = count_words, min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(8, 0.2) min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(8, 0.2), min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(50, 0.2), min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(50, 1), min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(200, 0.1), min.freq = 20)
wordlist <- str_split(ct$text[grep("carnival", ct$text, ignore.case = TRUE)], pattern = " ")
words_per_tweet <- sapply(wordlist, length)
all_words <- unlist(wordlist)
unique_words <- unique(all_words)
count_words_alt <- table(all_words)
count_words <- NULL
for (i in 1:length(unique_words)) {
count_words[i] = sum(all_words == unique_words[i])
names(count_words)[i] <- unique_words[i]
}
# head(count_words)
sort(count_words, decreasing = T )[1:20]
library(wordcloud)
wordcloud(unique_words, freq = count_words, scale = c(200, 0.1), min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(2, 0.1), min.freq = 20)
wordcloud(unique_words, freq = count_words, scale = c(2, 0.1), min.freq = 2)
wordcloud(unique_words, freq = count_words, scale = c(200, 0.1), min.freq = 2)
wordcloud(unique_words, freq = count_words, scale = c(50, 0.1), min.freq = 2)
wordcloud(unique_words, freq = count_words, scale = c(50, 0.1), min.freq = 2)
library(png)
library(grid)
grid.raster(img)
grid.raster(img)
```
img <- readPNG("~/repos/tweepy/Carnival/carnival-bounds.png")
grid.raster(img)
wordcloud(unique_words, freq = count_words, scale = c(50, 0.1), min.freq = 2)
qplot(words_per_tweet, geom = "histogram")
